





import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential









def load_data(train_dir, val_dir, test_dir,IMG_SIZE, BATCH_SIZE):
    train_dataset = tf.keras.utils.image_dataset_from_directory(train_directory,
                                                         shuffle=True,
                                                         batch_size=BATCH_SIZE,
                                                         image_size=IMG_SIZE )
    validation_dataset = tf.keras.utils.image_dataset_from_directory(valid_directory,
                                                         shuffle=True,
                                                         batch_size=BATCH_SIZE,
                                                         image_size=IMG_SIZE )
    test_dataset = tf.keras.utils.image_dataset_from_directory(test_directory, 
                                                         batch_size=BATCH_SIZE,
                                                         image_size=IMG_SIZE )
    return train_dataset,validation_dataset,test_dataset

BATCH_SIZE = 32
IMG_SIZE = (224,224)
train_directory = '/home/arsalan/DATA/github_files/keras_cv/TyreQualityPrediction/dataset/train'
valid_directory = '/home/arsalan/DATA/github_files/keras_cv/TyreQualityPrediction/dataset/valid'
test_directory = '/home/arsalan/DATA/github_files/keras_cv/TyreQualityPrediction/dataset/test'

train_dataset, validation_dataset, test_dataset = load_data(train_directory, valid_directory, test_directory, IMG_SIZE, BATCH_SIZE)





train_dataset_class_names = train_dataset.class_names
test_dataset_class_names = test_dataset.class_names
validation_dataset_class_names = validation_dataset.class_names

print(f"Class names in train dataset:{train_dataset_class_names}")
print(f"in test dataset:{test_dataset_class_names}")
print(f"and vlidation data set : {validation_dataset_class_names}")





plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1):
    print("Image batch shape:", images.shape)  
    print("Label batch shape:", labels.shape)
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(train_dataset_class_names[labels[i]])
        plt.axis("off")





data_augmentation = tf.keras.Sequential([
      tf.keras.layers.RandomFlip('horizontal'),
      tf.keras.layers.RandomRotation(0.2),
    ])


AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y)).cache().prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)














# Define the input shape including the number of color channels (3 for RGB)
IMG_SHAPE = IMG_SIZE + (3,)
preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')


base_model.trainable = False
base_model.summary()


def model(inputshape, preprocess_input,base_model):
    # Define the input layer with the specified input shape
    inputs = tf.keras.Input(shape=inputshape)
    
    # Preprocess the inputs using the MobileNetV2 preprocessing function
    x = preprocess_input(inputs)
    
    # Pass the preprocessed inputs through the base model (MobileNetV2)
    x = base_model(x, training=False)
    
    # Apply global average pooling to reduce the spatial dimensions
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    
    # Apply a dropout layer with a 20% dropout rate for regularization
    x = tf.keras.layers.Dropout(0.2)(x)
    
    # Add a dense output layer with a single neuron and sigmoid activation function for binary classification
    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)
    
    # Create the model by specifying the inputs and outputs
    model = tf.keras.Model(inputs, outputs)
    
    # Return the constructed model
    return model



# Create the model with the specified input shape and set the base model layers as non-trainable
model = model(IMG_SHAPE,preprocess_input,base_model)



model.summary()





callbacks = [
    # ModelCheckpoint callback saves the model after every epoch.
    # The filename includes the accuracy, validation accuracy, loss, and validation loss.
    tf.keras.callbacks.ModelCheckpoint(
        'model_accuracy:{accuracy:0.2f}_Val_acc:{val_accuracy:0.2f}_loss:{loss:0.2f}_Val_loss:{val_loss:0.2f}.keras', 
        save_best_only=True,  # Save only the best model based on the monitored metric.
        monitor='val_accuracy'  # Monitor the validation accuracy to determine the best model.
    ),
    # EarlyStopping callback stops training when the monitored metric has stopped improving.
    tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',  # Monitor the validation accuracy.
        patience= 8,  # Number of epochs with no improvement after which training will be stopped.
        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric.
    )
]



def compile_model(model, learning_rate):
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                  loss=tf.keras.losses.BinaryCrossentropy(),
                  metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])

                  
def fit_model(model, train_dataset, validation_dataset, initial_epochs , callbacks):                  
    history = model.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs, callbacks= callbacks)
    return history



learning_rate = 1e-4
initial_epochs = 20
compile_model(model, learning_rate)
history = fit_model(model, train_dataset, validation_dataset, initial_epochs ,callbacks)





def plot_history(history):
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.plot(history.history['val_accuracy'], label='val_accuracy')
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy / Loss')
    plt.legend(loc='lower right')
    plt.show()

plot_history(history)





def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.ylabel('Accuracy')
    plt.ylim([min(plt.ylim()),1])
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(2, 1, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.ylabel('Cross Entropy')
    plt.ylim([0,1.0])
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()
    return acc ,val_acc,loss,val_loss

acc ,val_acc,loss,val_loss = plot_history(history)





base_model.trainable = True


# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model.layers))





# Fine-tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False





model.summary()


learning_rate = 1e-5
initial_epochs = 20
compile_model(model, learning_rate)


# Define the number of epochs for fine-tuning
fine_tune_epochs = 20  # Number of epochs for fine-tuning after initial training

# Calculate the total number of epochs for the entire training process
total_epochs = initial_epochs + fine_tune_epochs

# Define a function to fit the model with fine-tuning
def fit_model_finetune(train_dataset, validation_dataset, initial_epochs, total_epochs, callbacks):
    # Fit the model using the training dataset
    # Start training from the last completed epoch in the initial training phase
    fine_tune_history = model.fit(
        train_dataset,
        epochs=total_epochs,  # Train until the total number of epochs is reached
        initial_epoch=len(history.epoch),  # Continue training from the epoch where initial training stopped
        validation_data=validation_dataset,  # Use the validation dataset for validation
        callbacks=callbacks  # Use the specified callbacks for saving the model and early stopping
    )
    return fine_tune_history  # Return the training history of the fine-tuning phase

# Perform fine-tuning on the model using the training and validation datasets
fine_tune_history = fit_model_finetune(train_dataset, validation_dataset, initial_epochs, total_epochs, callbacks)







acc += fine_tune_history.history['accuracy']
val_acc += fine_tune_history.history['val_accuracy']

loss += fine_tune_history.history['loss']
val_loss += fine_tune_history.history['val_loss']


plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()


def plot_history(history):
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.plot(history.history['val_accuracy'], label='val_accuracy')
    plt.plot(history.history['loss'], label='loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy / Loss')
    plt.legend(loc='lower right')
    plt.show()

plot_history(history)





loss, accuracy = model.evaluate(test_dataset)
print('Test accuracy :', accuracy)





# Retrieve a batch of images from the test set
image_batch, label_batch = test_dataset.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch).flatten()
predictions = tf.where(predictions < 0.5, 0, 1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)




plt.figure(figsize=(10, 10))
for i in range(9):
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image_batch[i].astype("uint8"))
  plt.title(test_dataset_class_names[predictions[i]])
  plt.axis("off")





from sklearn.metrics import confusion_matrix , classification_report
import seaborn as sns


# Initialize lists to accumulate predictions and labels
all_predictions = []
all_labels = []

# Iterate through the dataset
for image_batch, label_batch in test_dataset:
    predictions = model.predict_on_batch(image_batch).flatten()
    predictions = tf.where(predictions < 0.5, 0, 1)
    
    # Append current batch predictions and labels to the lists
    all_predictions.extend(predictions.numpy())
    all_labels.extend(label_batch.numpy())

# Convert lists to numpy arrays
all_predictions = np.array(all_predictions)
all_labels = np.array(all_labels)

# Print or visualize predictions and labels
print('All Predictions:\n', all_predictions)
print('All Labels:\n', all_labels)


len(all_predictions),len(all_labels)


cm = confusion_matrix(all_labels, all_predictions)
cm


# Plot confusion matrix with annotations for all values
plt.figure(figsize=(8, 8))
ax = sns.heatmap(cm, annot=False, fmt='d', cmap='Greens', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Add annotations for correct classifications
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}', ha='center', va='center', color='black', fontsize=8)

plt.show()


#Calculate and print accuracy for each class
class_accuracies = cm.diagonal() / cm.sum(axis=1)
for i, accuracy in enumerate(class_accuracies):
    print(f'Accuracy for class {i}: {accuracy:.2f}')


# Print classification report
print("Classification Report:")
print(classification_report(all_labels, all_predictions))







